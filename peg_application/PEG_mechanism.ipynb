{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from numpy.linalg import det\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9ec8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_oqwen_mmlu = pd.read_csv('/u/home/t/toz015/Agent/mmlu_policy_df_oqwen_7B.csv')\n",
    "temp_df_llama2_mmlu = pd.read_csv('/u/home/t/toz015/Agent/mmlu_policy_df_deepseek_llm_7b.csv')\n",
    "temp_df_qwen_mmlu = pd.read_csv('/u/home/t/toz015/Agent/mmlu_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "\n",
    "\n",
    "temp_df_oqwen = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_oqwen_7B.csv')\n",
    "temp_df_llama2 = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_deepseek_llm_7b.csv')\n",
    "temp_df_qwen = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "    \n",
    "temp_df_oqwen_arc_easy = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_easy_oqwen_7B.csv')\n",
    "temp_df_llama2_arc_easy = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_easy_deepseek_llm_7b.csv')\n",
    "temp_df_qwen_arc_easy = pd.read_csv('/u/home/t/toz015/Agent/arc_policy_df_easy_deepseek_qwen_7B.csv')\n",
    "\n",
    "\n",
    "\n",
    "temp_df_oqwen_gpqa = pd.read_csv('/u/home/t/toz015/Agent/gpqa_policy_df_oqwen_7B.csv')\n",
    "temp_df_llama2_gpqa = pd.read_csv('/u/home/t/toz015/Agent/gpqa_policy_df_deepseek_llm_7b.csv')\n",
    "temp_df_qwen_gpqa = pd.read_csv('/u/home/t/toz015/Agent/gpqa_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "# temp_df_oqwen_mmlu = pd.read_csv('Data/mmlu_policy_df_oqwen_7B.csv')\n",
    "# temp_df_llama2_mmlu = pd.read_csv('Data/mmlu_policy_df_deepseek_llm_7b.csv')\n",
    "# temp_df_qwen_mmlu = pd.read_csv('Data/mmlu_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "\n",
    "\n",
    "# temp_df_oqwen = pd.read_csv('Data/arc_policy_df_oqwen_7B.csv')\n",
    "# temp_df_llama2 = pd.read_csv('Data/arc_policy_df_deepseek_llm_7b.csv')\n",
    "# temp_df_qwen = pd.read_csv('Data/arc_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "    \n",
    "# temp_df_oqwen_arc_easy = pd.read_csv('Data/arc_policy_df_easy_oqwen_7B.csv')\n",
    "# temp_df_llama2_arc_easy = pd.read_csv('Data/arc_policy_df_easy_deepseek_llm_7b.csv')\n",
    "# temp_df_qwen_arc_easy = pd.read_csv('Data/arc_policy_df_easy_deepseek_qwen_7B.csv')\n",
    "\n",
    "\n",
    "\n",
    "# temp_df_oqwen_gpqa = pd.read_csv('Data/gpqa_policy_df_oqwen_7B.csv')\n",
    "# temp_df_llama2_gpqa = pd.read_csv('Data/gpqa_policy_df_deepseek_llm_7b.csv')\n",
    "# temp_df_qwen_gpqa = pd.read_csv('Data/gpqa_policy_df_deepseek_qwen_7B.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83af714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'subject', 'choices', 'correct_index', 'correct_letter',\n",
       "       'gen_init_answer', 'disc_answer', 'gen_answer', 'disc_init_answer',\n",
       "       'disc_final_policy_consensus', 'disc_init_policy', 'gen_init_policy',\n",
       "       'gen_final_policy_consensus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df_qwen_gpqa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb8f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df_qwen[\"answer_letter\"] = temp_df_qwen[\"answerKey\"]\n",
    "temp_df_oqwen[\"answer_letter\"] = temp_df_oqwen[\"answerKey\"]\n",
    "temp_df_llama2[\"answer_letter\"] = temp_df_llama2[\"answerKey\"]\n",
    "\n",
    "\n",
    "\n",
    "temp_df_qwen_arc_easy[\"answer_letter\"] = temp_df_qwen_arc_easy[\"answerKey\"]\n",
    "temp_df_oqwen_arc_easy[\"answer_letter\"] = temp_df_oqwen_arc_easy[\"answerKey\"]\n",
    "temp_df_llama2_arc_easy[\"answer_letter\"] = temp_df_llama2_arc_easy[\"answerKey\"]\n",
    "\n",
    "\n",
    "\n",
    "dict_map = {0:\"A\", 1:\"B\", 2: \"C\", 3: \"D\"}\n",
    "temp_df_qwen_mmlu[\"answer_letter\"] = temp_df_qwen_mmlu[\"answer\"].map(dict_map)\n",
    "temp_df_oqwen_mmlu[\"answer_letter\"] = temp_df_oqwen_mmlu[\"answer\"].map(dict_map)\n",
    "temp_df_llama2_mmlu[\"answer_letter\"] = temp_df_llama2_mmlu[\"answer\"].map(dict_map)\n",
    "\n",
    "\n",
    "temp_df_qwen_gpqa[\"answer_letter\"] = temp_df_qwen_gpqa[\"correct_letter\"]\n",
    "temp_df_oqwen_gpqa[\"answer_letter\"] = temp_df_oqwen_gpqa[\"correct_letter\"]\n",
    "temp_df_llama2_gpqa[\"answer_letter\"] = temp_df_llama2_gpqa[\"correct_letter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8ebd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_alignment(df_qwen, df_oqwen, df_llama2, answer_col='disc_init_answer', label_col='answer_letter'):\n",
    "    \"\"\"\n",
    "    Compare predicted answers and ground truth across three DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_qwen (DataFrame): First model's dataframe (e.g., Qwen).\n",
    "        df_oqwen (DataFrame): Second model's dataframe (e.g., Oqwen).\n",
    "        df_llama2 (DataFrame): Third model's dataframe (e.g., Llama2).\n",
    "        answer_col (str): Name of the model prediction column (default 'disc_init_answer').\n",
    "        label_col (str): Name of the ground truth column (default 'answer_letter').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Table summarizing counts and accuracies.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_n = df_oqwen.shape[0]\n",
    "    \n",
    "    qwen_corr = (df_qwen[answer_col] == df_qwen[label_col]).sum()\n",
    "    oqwen_corr = (df_oqwen[answer_col] == df_oqwen[label_col]).sum()\n",
    "    llama2_corr = (df_llama2[answer_col] == df_llama2[label_col]).sum()\n",
    "\n",
    "    qwen_llama2_oqwen = ((df_oqwen[answer_col] == df_llama2[answer_col]) & (df_llama2[answer_col] == df_llama2[label_col]) & (df_qwen[answer_col] == df_oqwen[answer_col])).sum()\n",
    "    \n",
    "    results_d = {\n",
    "        \"Metric\": [\n",
    "            \"Total Question #\",\n",
    "            \"oqwen correct answers\",\n",
    "            \"qwen correct answers\",\n",
    "            \"llama2 correct answers\",\n",
    "            \"qwen llama2 oqwen\", \n",
    "        ],\n",
    "        \"Count\": [\n",
    "            total_n,\n",
    "            oqwen_corr,\n",
    "            qwen_corr,\n",
    "            llama2_corr,\n",
    "            qwen_llama2_oqwen,\n",
    "        ]  \n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(results_d)\n",
    "    results_df[\"Accuracy\"] = (results_df[\"Count\"] / total_n).apply(lambda x: f\"{x:.0%}\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97411313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMMON_COLUMNS = [\"disc_init_policy\", \"gen_init_policy\", \"answer_letter\", \"disc_answer\", \"gen_answer\"]\n",
    "RENAME_MAPPING = {\"disc_answer\": \"ED_consensus\", \"gen_answer\": \"EG_consensus\"}\n",
    "\n",
    "def process_dataframe(temp_df):\n",
    "    \"\"\"Process a dataframe by selecting columns and renaming them.\"\"\"\n",
    "    df = temp_df[COMMON_COLUMNS].copy()\n",
    "    df.rename(columns=RENAME_MAPPING, inplace=True)\n",
    "    return df\n",
    "\n",
    "# ARC Challenge datasets\n",
    "df_oqwen = process_dataframe(temp_df_oqwen)\n",
    "df_deepseekqwen = process_dataframe(temp_df_qwen)\n",
    "df_deepseekllama = process_dataframe(temp_df_llama2)\n",
    "\n",
    "# ARC Easy datasets\n",
    "df_oqwen_arc_easy = process_dataframe(temp_df_oqwen_arc_easy)\n",
    "df_deepseekqwen_arc_easy = process_dataframe(temp_df_qwen_arc_easy)\n",
    "df_deepseekllama_arc_easy = process_dataframe(temp_df_llama2_arc_easy)\n",
    "\n",
    "# MMLU datasets\n",
    "df_oqwen_mmlu = process_dataframe(temp_df_oqwen_mmlu)\n",
    "df_deepseekqwen_mmlu = process_dataframe(temp_df_qwen_mmlu)\n",
    "df_deepseekllama_mmlu = process_dataframe(temp_df_llama2_mmlu)\n",
    "\n",
    "# GPQA datasets\n",
    "df_oqwen_gpqa = process_dataframe(temp_df_oqwen_gpqa)\n",
    "df_deepseekqwen_gpqa = process_dataframe(temp_df_qwen_gpqa)\n",
    "df_deepseekllama_gpqa = process_dataframe(temp_df_llama2_gpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df7362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_oqwen_mmlu, df_deepseekqwen_mmlu, df_deepseekllama_mmlu,\n",
    "    df_oqwen_arc_easy, df_deepseekqwen_arc_easy, df_deepseekllama_arc_easy,\n",
    "    df_oqwen_gpqa, df_deepseekqwen_gpqa, df_deepseekllama_gpqa\n",
    "]:\n",
    "    df[\"disc_init_policy\"] = df[\"disc_init_policy\"].apply(ast.literal_eval)\n",
    "    df[\"gen_init_policy\"] = df[\"gen_init_policy\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc917f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized the incorrect and correct policy in determinator\n",
    "def apply_softmax(choice_dict):\n",
    "    # Extract correct and incorrect values for all choices\n",
    "    \n",
    "    correct_values = np.array([v['correct'] for v in choice_dict.values()])\n",
    "    incorrect_values = np.array([v['incorrect'] for v in choice_dict.values()])\n",
    "    \n",
    "    # Define softmax function\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    softmax_correct = softmax(correct_values)\n",
    "    softmax_incorrect = softmax(incorrect_values)\n",
    "    \n",
    "    result = {}\n",
    "    for i, choice in enumerate(choice_dict.keys()):\n",
    "        result[choice] = {\n",
    "            'correct': softmax_correct[i],\n",
    "            'incorrect': softmax_incorrect[i]\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_oqwen_mmlu, df_deepseekqwen_mmlu, df_deepseekllama_mmlu,\n",
    "    df_oqwen_arc_easy, df_deepseekqwen_arc_easy, df_deepseekllama_arc_easy,\n",
    "    df_oqwen_gpqa, df_deepseekqwen_gpqa, df_deepseekllama_gpqa\n",
    "]:\n",
    "    df[\"disc_init_policy_refine\"] = df[\"disc_init_policy\"].apply(apply_softmax)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41b3904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_probable_letter(answer_dict):\n",
    "    \n",
    "    if answer_dict is None:\n",
    "        return None \n",
    "    \n",
    "    letter, _ = max(answer_dict.items(), key=lambda item: item[1]['correct'])\n",
    "    return letter\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_oqwen_mmlu, df_deepseekqwen_mmlu, df_deepseekllama_mmlu,\n",
    "    df_oqwen_arc_easy, df_deepseekqwen_arc_easy, df_deepseekllama_arc_easy,\n",
    "    df_oqwen_gpqa, df_deepseekqwen_gpqa, df_deepseekllama_gpqa\n",
    "]:\n",
    "    df[\"disc_init_answer\"] = df[\"disc_init_policy\"].apply(get_most_probable_letter)\n",
    "    df[\"disc_init_refine_answer\"] = df[\"disc_init_policy_refine\"].apply(get_most_probable_letter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e3321ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_joint(row):\n",
    "    plm_y_given_xl = row['gen_init_policy'] \n",
    "    plm_l_given_xy = row['disc_init_policy']  \n",
    "\n",
    "    new_joint = {}\n",
    "    for y, label_probs in plm_y_given_xl.items():\n",
    "        new_joint[y] = {}\n",
    "        for label, p1 in label_probs.items():\n",
    "            p2 = plm_l_given_xy.get(label, {}).get(y, 0.0)\n",
    "            new_joint[y][label] = p1 * p2\n",
    "    return new_joint\n",
    "\n",
    "\n",
    "def get_max_correct_label(mi_dict):\n",
    "    return max(mi_dict['correct'], key=mi_dict['correct'].get)\n",
    "\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_oqwen_mmlu, df_deepseekqwen_mmlu, df_deepseekllama_mmlu,\n",
    "    df_oqwen_arc_easy, df_deepseekqwen_arc_easy, df_deepseekllama_arc_easy,\n",
    "    df_oqwen_gpqa, df_deepseekqwen_gpqa, df_deepseekllama_gpqa\n",
    "]:\n",
    "    df[\"MI\"] = df.apply(reweight_joint, axis=1)\n",
    "    df[\"MI_answer\"] = df[\"MI\"].apply(get_max_correct_label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a25a47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_majority_vote_fixed(df_qwen, df_oqwen, df_llama2, answer_col='disc_init_refine_answer', label_col='answer_letter'):\n",
    "    \"\"\"\n",
    "    Compare three models' predictions with ground truth, and compute majority vote accuracy.\n",
    "    This function recalculates model correctness based on combined_df only.\n",
    "    \n",
    "    Args:\n",
    "        df_qwen (DataFrame): First model's dataframe (e.g., Qwen).\n",
    "        df_oqwen (DataFrame): Second model's dataframe (e.g., OQwen).\n",
    "        df_llama2 (DataFrame): Third model's dataframe (e.g., Llama2).\n",
    "        answer_col (str): Name of the model prediction column (default 'disc_init_refine_answer').\n",
    "        label_col (str): Name of the ground truth column (default 'answer_letter').\n",
    "        \n",
    "    Returns:\n",
    "        combined_df (DataFrame): DataFrame containing all predictions and majority votes.\n",
    "        results_df (DataFrame): Summary table of counts and accuracies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine predictions\n",
    "    combined_df = pd.DataFrame({\n",
    "        \"question_id\": df_oqwen.index,\n",
    "        \"correct_answer\": df_oqwen[label_col],   # Use OQwen's label (or consistent label)\n",
    "        \"qwen_pred\": df_qwen[answer_col],\n",
    "        \"oqwen_pred\": df_oqwen[answer_col],\n",
    "        \"llama2_pred\": df_llama2[answer_col]\n",
    "    })\n",
    "    \n",
    "    # Majority vote\n",
    "    def get_majority_vote(row):\n",
    "        votes = [row[\"qwen_pred\"], row[\"oqwen_pred\"], row[\"llama2_pred\"]]\n",
    "        vote_counts = Counter(votes)\n",
    "        majority_answers = [ans for ans, count in vote_counts.items() if count >= 2]\n",
    "        if not majority_answers:\n",
    "            return None\n",
    "        return majority_answers[0]\n",
    "    \n",
    "    combined_df[\"majority_vote\"] = combined_df.apply(get_majority_vote, axis=1)\n",
    "    \n",
    "    # Check correctness\n",
    "    combined_df[\"qwen_correct\"] = combined_df[\"qwen_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"oqwen_correct\"] = combined_df[\"oqwen_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"llama2_correct\"] = combined_df[\"llama2_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"is_majority_correct\"] = combined_df[\"majority_vote\"] == combined_df[\"correct_answer\"]\n",
    "\n",
    "    # Summarize\n",
    "    total_questions = len(combined_df)\n",
    "    qwen_corr = combined_df[\"qwen_correct\"].sum()\n",
    "    oqwen_corr = combined_df[\"oqwen_correct\"].sum()\n",
    "    llama2_corr = combined_df[\"llama2_correct\"].sum()\n",
    "    majority_correct = combined_df[\"is_majority_correct\"].sum()\n",
    "\n",
    "    results_d = {\n",
    "        \"Metric\": [\n",
    "            \"Total Questions\",\n",
    "            \"Qwen Accuracy\",\n",
    "            \"OQwen Accuracy\",\n",
    "            \"Llama2 Accuracy\",\n",
    "            \"Majority Vote (≥2 models) Accuracy\",\n",
    "        ],\n",
    "        \"Count\": [\n",
    "            total_questions,\n",
    "            qwen_corr,\n",
    "            oqwen_corr,\n",
    "            llama2_corr,\n",
    "            majority_correct,\n",
    "        ],\n",
    "        \"Accuracy\": [\n",
    "            \"100%\",\n",
    "            f\"{(qwen_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(oqwen_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(llama2_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(majority_correct / total_questions) * 100:.2f}%\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results_d)\n",
    "    \n",
    "    return combined_df, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70c92c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, init_policy):\n",
    "        \"\"\"Initialize with given probability dictionary.\"\"\"\n",
    "        self.policy = init_policy \n",
    "\n",
    "    def respond(self, choice, max_choice):\n",
    "        \"\"\"Sample a response ('correct' or 'incorrect') for a given answer choice.\"\"\"\n",
    "        prob_correct = self.policy[choice]['correct']\n",
    "        res = \"correct\" if choice == max_choice else \"incorrect\"    \n",
    "#         if choice == max_choice:\n",
    "#             res = \"correct\"\n",
    "#         else:\n",
    "#             res = np.random.choice([\"correct\", \"incorrect\"], p=[prob_correct, 1- prob_correct])\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def log_gradient(self, choice, response):\n",
    "\n",
    "        if response == \"correct\":\n",
    "            grad_correct = 1 / self.policy[choice]['correct']\n",
    "            grad_incorrect = -1 / self.policy[choice]['correct']\n",
    "        else:\n",
    "            grad_correct = -1 / self.policy[choice]['incorrect']\n",
    "            grad_incorrect = 1 / self.policy[choice]['incorrect']\n",
    "\n",
    "        return {'correct': grad_correct, 'incorrect': grad_incorrect}\n",
    "\n",
    "    def get_max_correct_choice(self):\n",
    "        \"\"\"Return the choice with highest 'correct' probability\"\"\"\n",
    "        return max(self.policy.items(), key=lambda x: x[1]['correct'])[0]\n",
    "    \n",
    "    def update_policy(self, choice, response, reward, learning_rate=0.1):\n",
    "        grads = self.log_gradient(choice, response)\n",
    "        prob_correct = self.policy[choice]['correct']\n",
    "        prob_incorrect = self.policy[choice]['incorrect']\n",
    "        \n",
    "        log_prob_correct = np.log(prob_correct) + learning_rate * reward * grads['correct']\n",
    "        log_prob_incorrect = np.log(prob_incorrect) + learning_rate * reward * grads['incorrect']\n",
    "\n",
    "        max_log = max(log_prob_correct, log_prob_incorrect)\n",
    "        log_prob_correct -= max_log\n",
    "        log_prob_incorrect -= max_log\n",
    "\n",
    "        exp_correct = np.exp(log_prob_correct)\n",
    "        exp_incorrect = np.exp(log_prob_incorrect)\n",
    "        total = exp_correct + exp_incorrect\n",
    "        self.policy[choice]['correct'] = exp_correct / total\n",
    "        self.policy[choice]['incorrect'] = exp_incorrect / total\n",
    "        \n",
    "        self.policy[choice]['correct'] = np.clip(self.policy[choice]['correct'], 1e-6, 1 - 1e-6)\n",
    "        self.policy[choice]['incorrect'] = 1 - self.policy[choice]['correct']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_to_index = {'correct': 0, 'incorrect': 1}\n",
    "\n",
    "def normalize_rewards(dmi_dict):\n",
    "    values = np.array(list(dmi_dict.values()))\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values) + 1e-8\n",
    "    return {k: (v - mean) / std for k, v in dmi_dict.items()}\n",
    "\n",
    "\n",
    "        \n",
    "def generate_matrix(part_data, i, j):\n",
    "    matrix = np.zeros((2, 2), dtype=int)\n",
    "    for row in part_data:\n",
    "        row_i = label_to_index[row[i]]\n",
    "        row_j = label_to_index[row[j]]\n",
    "        matrix[row_i][row_j] += 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_dmi_score(part1_matrix, part2_matrix):\n",
    "    det_part1 = np.linalg.det(part1_matrix)\n",
    "    det_part2 = np.linalg.det(part2_matrix)\n",
    "    dmi_score = det_part1 * det_part2\n",
    "    return dmi_score\n",
    "\n",
    "def compute_dmi_payments(responses):\n",
    "    \"\"\"Compute DMI payments using determinants of agreement matrices.\"\"\"\n",
    "    n = len(responses)\n",
    "    split1 = responses[ : (n + 1)//2]  \n",
    "    split2 = responses[(n + 1)//2 : ]  \n",
    "    splits = {'Part1': split1, 'Part2': split2}\n",
    "    \n",
    "    n_d = len(responses[0])\n",
    "    d_dmi_dict = {i : 0 for i in range(n_d)}\n",
    "    for i, j in combinations(range(n_d), 2):\n",
    "        part1_matrix = generate_matrix(splits[\"Part1\"], i, j)\n",
    "        part2_matrix = generate_matrix(splits[\"Part2\"], i, j)\n",
    "        dmi_score = calculate_dmi_score(part1_matrix, part2_matrix)\n",
    "        d_dmi_dict[i] += dmi_score\n",
    "        d_dmi_dict[j] += dmi_score\n",
    "        \n",
    "    return d_dmi_dict\n",
    "\n",
    "\n",
    "def batch_update_discriminators_flexible(\n",
    "    df_discriminator01, df_discriminator02, df_discriminator03,\n",
    "    choice_df,\n",
    "    batch_size=8,\n",
    "    T_steps=10,\n",
    "    learning_rate=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Batch update discriminator policies for three dataframes.\n",
    "    \n",
    "    Args:\n",
    "        df_discriminator01 (DataFrame): DataFrame containing 'disc_init_policy_refine' for OQwen.\n",
    "        df_discriminator02 (DataFrame): DataFrame containing 'disc_init_policy_refine' for DeepseekQwen.\n",
    "        df_discriminator03 (DataFrame): DataFrame containing 'disc_init_policy_refine' for DeepseekLlama.\n",
    "        choice_df (DataFrame): DataFrame used for selecting initial choices (e.g., DeepseekQwen's 'gen_init_policy').\n",
    "        batch_size (int): Number of rows to update in each batch.\n",
    "        T_steps (int): Number of response-update iterations per batch.\n",
    "        learning_rate (float): Learning rate for discriminator policy update.\n",
    "        \n",
    "    Returns:\n",
    "        Updated DataFrames (inplace modification too).\n",
    "    \"\"\"\n",
    "    if batch_size < 4:\n",
    "        print(\"batch size should larger than 4\")\n",
    "    # Initialize updated_disc_policy columns\n",
    "    for df in [df_discriminator01, df_discriminator02, df_discriminator03]:\n",
    "        df[\"updated_disc_policy\"] = None\n",
    "\n",
    "    n_rows = df_discriminator01.shape[0]\n",
    "    \n",
    "    for i in range(0, n_rows - batch_size, batch_size):\n",
    "        # Initialize discriminators\n",
    "        discriminators = [\n",
    "            [Discriminator(copy.deepcopy(df_discriminator01.loc[i + j, 'disc_init_policy_refine'])) for j in range(batch_size)],\n",
    "            [Discriminator(copy.deepcopy(df_discriminator02.loc[i + j, 'disc_init_policy_refine'])) for j in range(batch_size)],\n",
    "            [Discriminator(copy.deepcopy(df_discriminator03.loc[i + j, 'disc_init_policy_refine'])) for j in range(batch_size)]\n",
    "        ]\n",
    "\n",
    "        # Initialize choices based on the passed-in choice_df\n",
    "        choices = [\n",
    "            max(choice_df.loc[i + j, \"gen_init_policy\"][\"correct\"],\n",
    "                key=choice_df.loc[i + j, \"gen_init_policy\"][\"correct\"].get)\n",
    "            for j in range(batch_size)\n",
    "        ]\n",
    "\n",
    "        responses = []\n",
    "        for _ in range(T_steps):\n",
    "            batch_responses = []\n",
    "            for j in range(batch_size):\n",
    "                task_responses = []\n",
    "                for d in range(3):\n",
    "                    max_choice = discriminators[d][j].get_max_correct_choice()\n",
    "                    task_responses.append(discriminators[d][j].respond(choices[j], max_choice))\n",
    "                batch_responses.append(task_responses)\n",
    "            responses = batch_responses\n",
    "\n",
    "            dmi_scores = compute_dmi_payments(batch_responses)\n",
    "            dmi_scores = normalize_rewards(dmi_scores)\n",
    "\n",
    "            # Update discriminator policies\n",
    "            for d in range(3):\n",
    "                for j in range(batch_size):\n",
    "                    choice = choices[j]\n",
    "                    response = batch_responses[j][d]\n",
    "                    reward = dmi_scores[d]\n",
    "                    discriminators[d][j].update_policy(choice, response, reward, learning_rate)\n",
    "\n",
    "        # Save updated policies\n",
    "        for j in range(batch_size):\n",
    "            df_discriminator01.at[i + j, \"updated_disc_policy\"] = discriminators[0][j].policy\n",
    "            df_discriminator02.at[i + j, \"updated_disc_policy\"] = discriminators[1][j].policy\n",
    "            df_discriminator03.at[i + j, \"updated_disc_policy\"] = discriminators[2][j].policy\n",
    "    \n",
    "    \n",
    "    df_discriminator01['updated_answer_letter'] = df_discriminator01['updated_disc_policy'].apply(get_most_probable_letter)\n",
    "    df_discriminator02['updated_answer_letter'] = df_discriminator02['updated_disc_policy'].apply(get_most_probable_letter)\n",
    "    df_discriminator03['updated_answer_letter'] = df_discriminator03['updated_disc_policy'].apply(get_most_probable_letter)\n",
    "\n",
    "\n",
    "    return df_discriminator01, df_discriminator02, df_discriminator03\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d54199f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>820</td>\n",
       "      <td>70.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1020</td>\n",
       "      <td>87.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>725</td>\n",
       "      <td>61.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>901</td>\n",
       "      <td>77.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                       Qwen Accuracy    820   70.09%\n",
       "2                      OQwen Accuracy   1020   87.18%\n",
       "3                     Llama2 Accuracy    725   61.97%\n",
       "4  Majority Vote (≥2 models) Accuracy    901   77.01%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df_inital_gen, results_df_inital_gen = evaluate_majority_vote_fixed(temp_df_qwen, temp_df_oqwen, temp_df_llama2,  answer_col='gen_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1758c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC Challenge\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oqwen correct answers</td>\n",
       "      <td>970</td>\n",
       "      <td>83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen correct answers</td>\n",
       "      <td>669</td>\n",
       "      <td>57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2 correct answers</td>\n",
       "      <td>700</td>\n",
       "      <td>60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen llama2 oqwen</td>\n",
       "      <td>457</td>\n",
       "      <td>39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Count Accuracy\n",
       "0        Total Question #   1170     100%\n",
       "1   oqwen correct answers    970      83%\n",
       "2    qwen correct answers    669      57%\n",
       "3  llama2 correct answers    700      60%\n",
       "4       qwen llama2 oqwen    457      39%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_table = evaluate_answer_alignment(temp_df_qwen, temp_df_oqwen, temp_df_llama2)\n",
    "\n",
    "print(\"ARC Challenge\")\n",
    "display(results_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "955c83f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>669</td>\n",
       "      <td>57.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>970</td>\n",
       "      <td>82.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>700</td>\n",
       "      <td>59.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>827</td>\n",
       "      <td>70.68%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                       Qwen Accuracy    669   57.18%\n",
       "2                      OQwen Accuracy    970   82.91%\n",
       "3                     Llama2 Accuracy    700   59.83%\n",
       "4  Majority Vote (≥2 models) Accuracy    827   70.68%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Challenger Original answer \n",
    "\n",
    "combined_df_inital, results_df_inital = evaluate_majority_vote_fixed(df_deepseekqwen, df_oqwen, df_deepseekllama,  answer_col='disc_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d118a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1027</td>\n",
       "      <td>87.78%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>748</td>\n",
       "      <td>63.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>913</td>\n",
       "      <td>78.03%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                       Qwen Accuracy    822   70.26%\n",
       "2                      OQwen Accuracy   1027   87.78%\n",
       "3                     Llama2 Accuracy    748   63.93%\n",
       "4  Majority Vote (≥2 models) Accuracy    913   78.03%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Challenger Mutual Information Ranking\n",
    "combined_df_MI, results_df_MI = evaluate_majority_vote_fixed(df_deepseekqwen, df_oqwen,  df_deepseekllama,  answer_col='MI_answer', label_col='answer_letter')\n",
    "display(results_df_MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12b1053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1025</td>\n",
       "      <td>87.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>741</td>\n",
       "      <td>63.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>907</td>\n",
       "      <td>77.52%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                       Qwen Accuracy    822   70.26%\n",
       "2                      OQwen Accuracy   1025   87.61%\n",
       "3                     Llama2 Accuracy    741   63.33%\n",
       "4  Majority Vote (≥2 models) Accuracy    907   77.52%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Challenger ED consensus\n",
    "\n",
    "combined_df_ED, results_df_ED = evaluate_majority_vote_fixed(df_deepseekqwen, df_oqwen, df_deepseekllama,  answer_col='ED_consensus', label_col='answer_letter')\n",
    "display(results_df_ED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d911bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>1008</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>995</td>\n",
       "      <td>85.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>1006</td>\n",
       "      <td>85.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>1018</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                       Qwen Accuracy   1008   86.15%\n",
       "2                      OQwen Accuracy    995   85.04%\n",
       "3                     Llama2 Accuracy   1006   85.98%\n",
       "4  Majority Vote (≥2 models) Accuracy   1018   87.01%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Challenger Original Qwen as problem selector\n",
    "\n",
    "df_oqwen, df_deepseekqwen, df_deepseekllama =  batch_update_discriminators_flexible(\n",
    "    df_oqwen, \n",
    "    df_deepseekqwen, \n",
    "    df_deepseekllama,\n",
    "    choice_df = df_oqwen, \n",
    "    batch_size=8,\n",
    "    T_steps=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_update, results_df_refine_update = evaluate_majority_vote_fixed(df_deepseekqwen, df_oqwen, df_deepseekllama,  answer_col='updated_answer_letter', label_col='answer_letter')\n",
    "display(results_df_refine_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a702f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC Easy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oqwen correct answers</td>\n",
       "      <td>2144</td>\n",
       "      <td>90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen correct answers</td>\n",
       "      <td>1671</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2 correct answers</td>\n",
       "      <td>1809</td>\n",
       "      <td>76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen llama2 oqwen</td>\n",
       "      <td>1415</td>\n",
       "      <td>60%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Count Accuracy\n",
       "0        Total Question #   2371     100%\n",
       "1   oqwen correct answers   2144      90%\n",
       "2    qwen correct answers   1671      70%\n",
       "3  llama2 correct answers   1809      76%\n",
       "4       qwen llama2 oqwen   1415      60%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "results_table_arc_easy = evaluate_answer_alignment(temp_df_qwen_arc_easy, temp_df_oqwen_arc_easy, temp_df_llama2_arc_easy)\n",
    "\n",
    "print(\"ARC Easy\")\n",
    "display(results_table_arc_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6bdbe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>1935</td>\n",
       "      <td>81.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>2180</td>\n",
       "      <td>91.94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>1831</td>\n",
       "      <td>77.22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>2091</td>\n",
       "      <td>88.19%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   2371     100%\n",
       "1                       Qwen Accuracy   1935   81.61%\n",
       "2                      OQwen Accuracy   2180   91.94%\n",
       "3                     Llama2 Accuracy   1831   77.22%\n",
       "4  Majority Vote (≥2 models) Accuracy   2091   88.19%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "combined_df_inital_gen, results_df_inital_gen = evaluate_majority_vote_fixed(temp_df_qwen_arc_easy, temp_df_oqwen_arc_easy, temp_df_llama2_arc_easy,  answer_col='gen_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1b345ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>1671</td>\n",
       "      <td>70.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>2144</td>\n",
       "      <td>90.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>1809</td>\n",
       "      <td>76.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>1996</td>\n",
       "      <td>84.18%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   2371     100%\n",
       "1                       Qwen Accuracy   1671   70.48%\n",
       "2                      OQwen Accuracy   2144   90.43%\n",
       "3                     Llama2 Accuracy   1809   76.30%\n",
       "4  Majority Vote (≥2 models) Accuracy   1996   84.18%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Easy Original answer \n",
    "\n",
    "combined_df_inital_arc_easy, results_df_inital_arc_easy = evaluate_majority_vote_fixed(df_deepseekqwen_arc_easy, df_oqwen_arc_easy, df_deepseekllama_arc_easy,  answer_col='disc_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_arc_easy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0b3f8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>1942</td>\n",
       "      <td>81.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>2190</td>\n",
       "      <td>92.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>1853</td>\n",
       "      <td>78.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>2101</td>\n",
       "      <td>88.61%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   2371     100%\n",
       "1                       Qwen Accuracy   1942   81.91%\n",
       "2                      OQwen Accuracy   2190   92.37%\n",
       "3                     Llama2 Accuracy   1853   78.15%\n",
       "4  Majority Vote (≥2 models) Accuracy   2101   88.61%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Easy Mutual Information Ranking\n",
    "combined_df_MI_arc_easy, results_df_MI_arc_easy = evaluate_majority_vote_fixed(df_deepseekqwen_arc_easy, df_oqwen_arc_easy, df_deepseekllama_arc_easy,  answer_col='MI_answer', label_col='answer_letter')\n",
    "display(results_df_MI_arc_easy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55eeb9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>1940</td>\n",
       "      <td>81.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>2188</td>\n",
       "      <td>92.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>1845</td>\n",
       "      <td>77.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>2100</td>\n",
       "      <td>88.57%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   2371     100%\n",
       "1                       Qwen Accuracy   1940   81.82%\n",
       "2                      OQwen Accuracy   2188   92.28%\n",
       "3                     Llama2 Accuracy   1845   77.82%\n",
       "4  Majority Vote (≥2 models) Accuracy   2100   88.57%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Easy Consensus ED answer\n",
    "\n",
    "combined_df_ED_arc_easy, results_df_ED_arc_easy = evaluate_majority_vote_fixed(df_deepseekqwen_arc_easy, df_oqwen_arc_easy,  df_deepseekllama_arc_easy,  answer_col='ED_consensus', label_col='answer_letter')\n",
    "display(results_df_ED_arc_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50813745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>2371</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>2175</td>\n",
       "      <td>91.73%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>2155</td>\n",
       "      <td>90.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>2168</td>\n",
       "      <td>91.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>2176</td>\n",
       "      <td>91.78%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   2371     100%\n",
       "1                       Qwen Accuracy   2175   91.73%\n",
       "2                      OQwen Accuracy   2155   90.89%\n",
       "3                     Llama2 Accuracy   2168   91.44%\n",
       "4  Majority Vote (≥2 models) Accuracy   2176   91.78%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ARC Easy Original Qwen as problem selector\n",
    "\n",
    "df_oqwen_arc_easy, df_deepseekqwen_arc_easy, df_deepseekllama_arc_easy =  batch_update_discriminators_flexible(\n",
    "    df_oqwen_arc_easy, \n",
    "    df_deepseekqwen_arc_easy, \n",
    "    df_deepseekllama_arc_easy,\n",
    "    choice_df = df_oqwen_arc_easy, \n",
    "    batch_size=8,\n",
    "    T_steps=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_update_arc_easy, results_df_refine_update_arc_easy = evaluate_majority_vote_fixed( df_deepseekqwen_arc_easy, df_oqwen_arc_easy,df_deepseekllama_arc_easy,  answer_col='updated_answer_letter', label_col='answer_letter')\n",
    "display(results_df_refine_update_arc_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8eaaad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>7404</td>\n",
       "      <td>53.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>9849</td>\n",
       "      <td>71.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>6978</td>\n",
       "      <td>50.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>8316</td>\n",
       "      <td>59.96%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions  13869     100%\n",
       "1                       Qwen Accuracy   7404   53.39%\n",
       "2                      OQwen Accuracy   9849   71.01%\n",
       "3                     Llama2 Accuracy   6978   50.31%\n",
       "4  Majority Vote (≥2 models) Accuracy   8316   59.96%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df_inital_gen, results_df_inital_gen = evaluate_majority_vote_fixed(temp_df_qwen_mmlu, temp_df_oqwen_mmlu, temp_df_llama2_mmlu,  answer_col='gen_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93096dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oqwen correct answers</td>\n",
       "      <td>8698</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen correct answers</td>\n",
       "      <td>5878</td>\n",
       "      <td>42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2 correct answers</td>\n",
       "      <td>6411</td>\n",
       "      <td>46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen llama2 oqwen</td>\n",
       "      <td>2957</td>\n",
       "      <td>21%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Count Accuracy\n",
       "0        Total Question #  13869     100%\n",
       "1   oqwen correct answers   8698      63%\n",
       "2    qwen correct answers   5878      42%\n",
       "3  llama2 correct answers   6411      46%\n",
       "4       qwen llama2 oqwen   2957      21%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_table_mmlu = evaluate_answer_alignment(temp_df_qwen_mmlu, temp_df_oqwen_mmlu, temp_df_llama2_mmlu)\n",
    "\n",
    "print(\"MMLU\")\n",
    "display(results_table_mmlu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d9514445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>5878</td>\n",
       "      <td>42.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>8698</td>\n",
       "      <td>62.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>6411</td>\n",
       "      <td>46.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>7038</td>\n",
       "      <td>50.75%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions  13869     100%\n",
       "1                       Qwen Accuracy   5878   42.38%\n",
       "2                      OQwen Accuracy   8698   62.72%\n",
       "3                     Llama2 Accuracy   6411   46.23%\n",
       "4  Majority Vote (≥2 models) Accuracy   7038   50.75%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## MMLU Original answer\n",
    "combined_df_inital_mmlu, results_df_inital_mmlu = evaluate_majority_vote_fixed(df_deepseekqwen_mmlu, df_oqwen_mmlu, df_deepseekllama_mmlu,  answer_col='disc_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_mmlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fbcbde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>7435</td>\n",
       "      <td>53.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>9830</td>\n",
       "      <td>70.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>6949</td>\n",
       "      <td>50.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>8301</td>\n",
       "      <td>59.85%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions  13869     100%\n",
       "1                       Qwen Accuracy   7435   53.61%\n",
       "2                      OQwen Accuracy   9830   70.88%\n",
       "3                     Llama2 Accuracy   6949   50.10%\n",
       "4  Majority Vote (≥2 models) Accuracy   8301   59.85%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## MMLU Mutual Information Ranking\n",
    "combined_df_MI_mmlu, results_df_MI_mmlu = evaluate_majority_vote_fixed(df_deepseekqwen_mmlu, df_oqwen_mmlu, df_deepseekllama_mmlu,  answer_col='MI_answer', label_col='answer_letter')\n",
    "display(results_df_MI_mmlu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d697aad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>7430</td>\n",
       "      <td>53.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>9761</td>\n",
       "      <td>70.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>6966</td>\n",
       "      <td>50.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>8274</td>\n",
       "      <td>59.66%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions  13869     100%\n",
       "1                       Qwen Accuracy   7430   53.57%\n",
       "2                      OQwen Accuracy   9761   70.38%\n",
       "3                     Llama2 Accuracy   6966   50.23%\n",
       "4  Majority Vote (≥2 models) Accuracy   8274   59.66%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## MMLU Consensus ED answer\n",
    "\n",
    "combined_df_ED_mmlu, results_df_ED_mmlu = evaluate_majority_vote_fixed(df_deepseekqwen_mmlu, df_oqwen_mmlu, df_deepseekllama_mmlu,  answer_col='ED_consensus', label_col='answer_letter')\n",
    "display(results_df_ED_mmlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12e8fe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>9575</td>\n",
       "      <td>69.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>9457</td>\n",
       "      <td>68.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>9530</td>\n",
       "      <td>68.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>9809</td>\n",
       "      <td>70.73%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions  13869     100%\n",
       "1                       Qwen Accuracy   9575   69.04%\n",
       "2                      OQwen Accuracy   9457   68.19%\n",
       "3                     Llama2 Accuracy   9530   68.71%\n",
       "4  Majority Vote (≥2 models) Accuracy   9809   70.73%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## l Qwen as problem selector\n",
    "\n",
    "df_oqwen_mmlu, df_deepseekqwen_mmlu, df_deepseekllama_mmlu =  batch_update_discriminators_flexible(\n",
    "    df_oqwen_mmlu, \n",
    "    df_deepseekqwen_mmlu, \n",
    "    df_deepseekllama_mmlu,\n",
    "    choice_df = df_oqwen_mmlu, \n",
    "    batch_size=8,\n",
    "    T_steps=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_update_mmlu, results_df_refine_update_mmlu = evaluate_majority_vote_fixed(df_deepseekqwen_mmlu, df_oqwen_mmlu,  df_deepseekllama_mmlu,  answer_col='updated_answer_letter', label_col='answer_letter')\n",
    "display(results_df_refine_update_mmlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "46ac6aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>82</td>\n",
       "      <td>18.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>101</td>\n",
       "      <td>22.54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>88</td>\n",
       "      <td>19.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>81</td>\n",
       "      <td>18.08%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     82   18.30%\n",
       "2                      OQwen Accuracy    101   22.54%\n",
       "3                     Llama2 Accuracy     88   19.64%\n",
       "4  Majority Vote (≥2 models) Accuracy     81   18.08%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "combined_df_inital_gen, results_df_inital_gen = evaluate_majority_vote_fixed(temp_df_qwen_gpqa, temp_df_oqwen_gpqa, temp_df_llama2_gpqa,  answer_col='gen_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bbfe90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>68</td>\n",
       "      <td>15.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>72</td>\n",
       "      <td>16.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>78</td>\n",
       "      <td>17.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>41</td>\n",
       "      <td>9.15%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     68   15.18%\n",
       "2                      OQwen Accuracy     72   16.07%\n",
       "3                     Llama2 Accuracy     78   17.41%\n",
       "4  Majority Vote (≥2 models) Accuracy     41    9.15%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## GPQA\n",
    "combined_df_inital_gpqa, results_df_inital_gpqa = evaluate_majority_vote_fixed(df_deepseekqwen_gpqa, df_oqwen_gpqa,  df_deepseekllama_gpqa,  answer_col='disc_init_answer', label_col='answer_letter')\n",
    "display(results_df_inital_gpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7edd5c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>68</td>\n",
       "      <td>15.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>72</td>\n",
       "      <td>16.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>78</td>\n",
       "      <td>17.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>41</td>\n",
       "      <td>9.15%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     68   15.18%\n",
       "2                      OQwen Accuracy     72   16.07%\n",
       "3                     Llama2 Accuracy     78   17.41%\n",
       "4  Majority Vote (≥2 models) Accuracy     41    9.15%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## GPQA Initial answer\n",
    "combined_df_refine_inital_gpqa, results_df_refine_inital_gpqa = evaluate_majority_vote_fixed(df_deepseekqwen_gpqa, df_oqwen_gpqa, df_deepseekllama_gpqa,  answer_col='disc_init_refine_answer', label_col='answer_letter')\n",
    "display(results_df_refine_inital_gpqa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bba819e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>83</td>\n",
       "      <td>18.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>88</td>\n",
       "      <td>19.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>89</td>\n",
       "      <td>19.87%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>73</td>\n",
       "      <td>16.29%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     83   18.53%\n",
       "2                      OQwen Accuracy     88   19.64%\n",
       "3                     Llama2 Accuracy     89   19.87%\n",
       "4  Majority Vote (≥2 models) Accuracy     73   16.29%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## GPQA Mutual Information Ranking\n",
    "combined_df_MI_gpqa, results_df_MI_gpqa = evaluate_majority_vote_fixed(df_deepseekqwen_gpqa, df_oqwen_gpqa, df_deepseekllama_gpqa,  answer_col='MI_answer', label_col='answer_letter')\n",
    "display(results_df_MI_gpqa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d541de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>84</td>\n",
       "      <td>18.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>90</td>\n",
       "      <td>20.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>85</td>\n",
       "      <td>18.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>74</td>\n",
       "      <td>16.52%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     84   18.75%\n",
       "2                      OQwen Accuracy     90   20.09%\n",
       "3                     Llama2 Accuracy     85   18.97%\n",
       "4  Majority Vote (≥2 models) Accuracy     74   16.52%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## GPQA Consensus ED answer\n",
    "\n",
    "combined_df_ED_gpqa, results_df_ED_gpqa = evaluate_majority_vote_fixed(df_deepseekqwen_gpqa, df_oqwen_gpqa,  df_deepseekllama_gpqa,  answer_col='ED_consensus', label_col='answer_letter')\n",
    "display(results_df_ED_gpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a224389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>448</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen Accuracy</td>\n",
       "      <td>90</td>\n",
       "      <td>20.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>102</td>\n",
       "      <td>22.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2 Accuracy</td>\n",
       "      <td>106</td>\n",
       "      <td>23.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (≥2 models) Accuracy</td>\n",
       "      <td>101</td>\n",
       "      <td>22.54%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions    448     100%\n",
       "1                       Qwen Accuracy     90   20.09%\n",
       "2                      OQwen Accuracy    102   22.77%\n",
       "3                     Llama2 Accuracy    106   23.66%\n",
       "4  Majority Vote (≥2 models) Accuracy    101   22.54%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## GPQA DMI Qwen as problem selector\n",
    "\n",
    "df_oqwen_gpqa, df_deepseekqwen_gpqa, df_deepseekllama_gpqa =  batch_update_discriminators_flexible(\n",
    "    df_oqwen_gpqa, \n",
    "    df_deepseekqwen_gpqa, \n",
    "    df_deepseekllama_gpqa,\n",
    "    choice_df = df_oqwen_gpqa, \n",
    "    batch_size=8,\n",
    "    T_steps=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_update_gpqa, results_df_refine_update_gpqa = evaluate_majority_vote_fixed(df_deepseekqwen_gpqa, df_oqwen_gpqa,  df_deepseekllama_gpqa,  answer_col='updated_answer_letter', label_col='answer_letter')\n",
    "display(results_df_refine_update_gpqa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
