{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f385e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"hf_***REDACTED***\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "hf_token = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea547b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpqa_data = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\")\n",
    "\n",
    "gpqa_df_raw = gpqa_data[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5730b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpqa_df_raw[['Question',\n",
    "       'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2',\n",
    "       'Incorrect Answer 3', 'Subdomain']].head(1)\n",
    "gpqa_df = pd.DataFrame({\n",
    "    'question': gpqa_df_raw['Question'],\n",
    "    'subject': gpqa_df_raw['Subdomain'],\n",
    "    'choices': gpqa_df_raw.apply(lambda row: [\n",
    "        row['Correct Answer'],\n",
    "        row['Incorrect Answer 1'],\n",
    "        row['Incorrect Answer 2'],\n",
    "        row['Incorrect Answer 3']\n",
    "    ], axis=1),\n",
    "    'correct_index': 1  \n",
    "})\n",
    "\n",
    "\n",
    "def shuffle_choices(row):\n",
    "    choices = row['choices'].copy()\n",
    "    correct = choices[0]  \n",
    "    random.shuffle(choices)\n",
    "    new_index = choices.index(correct) + 1  # 1-based index\n",
    "    return pd.Series([choices, new_index])\n",
    "\n",
    "gpqa_df[['choices', 'correct_index']] = gpqa_df.apply(shuffle_choices, axis=1)\n",
    "gpqa_df['correct_letter'] = gpqa_df['correct_index'].apply(lambda x: chr(65 + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb3c5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "import accelerate\n",
    "print(accelerate.__version__)  # Should show â‰¥0.26.0\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c13cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# 1. INITIAL GENERATOR POLICIES\n",
    "################################\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_generator_prompt(\n",
    "    subject,\n",
    "    target_question,\n",
    "    target_choices,\n",
    "    get_correct\n",
    "):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject))\n",
    "\n",
    "    prompt += f\"{target_question}\"\n",
    "    for i, c in enumerate(target_choices):\n",
    "        prompt += \"\\n{}\".format(c)\n",
    "        \n",
    "    if get_correct:\n",
    "        prompt += \"\\nAnswer:\"\n",
    "    else:\n",
    "        prompt += \"\\nIncorrect Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_logits = []\n",
    "    for letter in choices:\n",
    "        token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
    "        choice_logits.append(logits[token_id].item())\n",
    "    \n",
    "    \n",
    "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    choice_probs =  {choice: prob for choice, prob in zip(choices, probs)}\n",
    "    \n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "\n",
    "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
    "    # Generate the letter answer\n",
    "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
    "\n",
    "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
    "    \n",
    "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
    "    \n",
    "    return probs \n",
    "\n",
    "\n",
    "\n",
    "def get_initial_generator_probs(row, model, tokenizer):\n",
    "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
    "    for v in [True, False]:\n",
    "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
    "        if v:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"correct\"][key] = val\n",
    "                #print(gen_init.items())\n",
    "        else:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"incorrect\"][key] = val\n",
    "\n",
    "    return gen_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de33cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 2. INITIAL DISCRIMINATOR POLICIES\n",
    "###################################\n",
    "\n",
    "def build_discriminator_prompt(\n",
    "    subject: str,\n",
    "    question: str,\n",
    "    proposed_answer: str\n",
    ") -> str:\n",
    "    \"\"\"Builds a prompt to evaluate answer correctness.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    \n",
    "def get_discriminator_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    choices_list\n",
    "):\n",
    "    input_ids = input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    choice_logits = torch.tensor(\n",
    "        [\n",
    "            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "        ]\n",
    "    ).float()\n",
    "    \n",
    "    disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_probs =  {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
    "\n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "def evaluate_answer_correctness(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"Evaluates all possible answers for a question.\"\"\"\n",
    "    subject = row[\"subject\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for idx, answer in enumerate(choices):\n",
    "        prompt = build_discriminator_prompt(\n",
    "            subject=subject,\n",
    "            question=question,\n",
    "            proposed_answer=f\"{answer}\"\n",
    "        )\n",
    "        \n",
    "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
    "        \n",
    "        \n",
    "        disc_dict_answer =  {i: f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])}\n",
    "        \n",
    "        \n",
    "        results[disc_dict_answer[idx]] = probs\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "def get_initial_discriminator_probs(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
    "    \n",
    "\n",
    "    return disc_init\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8675f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
    "    \"\"\"\n",
    "    method='generator': pick argmax_y pi_G(correct|y)\n",
    "    method='discriminator': pick argmax_y pi_D(correct|y)\n",
    "    \"\"\"\n",
    "    if method == \"generator\":\n",
    "        # For each candidate y, we look at gen[\"correct\"][y].\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen[\"correct\"][y]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "    else:\n",
    "        # method='discriminator'\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc[y][\"correct\"]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(arr):\n",
    "    \"\"\"Numerically stable softmax over a 1D numpy array.\"\"\"\n",
    "    m = np.max(arr)\n",
    "    exp_vals = np.exp(arr - m)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "\n",
    "def equilibrium_search(gen_init, disc_init, \n",
    "                       candidates, \n",
    "                       T=5000, \n",
    "                       eta_G=0.1, eta_D=0.1, \n",
    "                       lam_G=0.1, lam_D=0.01):\n",
    "    \"\"\"\n",
    "    Runs iterative no-regret policy updates to find approximate equilibrium.\n",
    "    gen_init, disc_init: dictionary form from the above initialization steps.\n",
    "    \"\"\"\n",
    "    gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "    disc = {}\n",
    "    for y in candidates:\n",
    "        disc[y] = dict(disc_init[y])  # copy\n",
    "\n",
    "    Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        # 1) Update Q\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            for y in candidates:\n",
    "                \n",
    "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "        for y in candidates:\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                \n",
    "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "        # 2) Update generator policy\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            logits = []\n",
    "            for y in candidates:\n",
    "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12) )/ (1/eta_G  + lam_G)\n",
    "                logits.append(val)\n",
    "\n",
    "            new_probs = softmax(np.array(logits))\n",
    "\n",
    "            for i, y in enumerate(candidates):\n",
    "                gen[v][y] = new_probs[i]\n",
    "                \n",
    "        logits_correct = []\n",
    "        logits_incorrect = []\n",
    "        for y in candidates:\n",
    "            # Logit for \"correct\"\n",
    "            val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_correct.append(val_correct)\n",
    "\n",
    "            # Logit for \"incorrect\"\n",
    "            val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_incorrect.append(val_incorrect)\n",
    "\n",
    "        new_probs_correct = softmax(np.array(logits_correct))\n",
    "        new_probs_incorrect = softmax(np.array(logits_incorrect))\n",
    "\n",
    "        for i, y in enumerate(candidates):\n",
    "            disc[y][\"correct\"] = new_probs_correct[i]\n",
    "            disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
    "\n",
    "    return gen, disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf89481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load one model at a time with 4-bit quantization\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9efebe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcategory_df_function(model_d, tokenizer_d, df):\n",
    "    \n",
    "    category_df = df.copy()\n",
    "\n",
    "    ## Final\n",
    "    gen_answer = []\n",
    "    disc_answer = []\n",
    "\n",
    "    ## Generative Initial result\n",
    "    gen_init_answer = []\n",
    "\n",
    "    disc_init_answer = []\n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_final_policy_consensus = []\n",
    "    gen_final_policy_consensus = []\n",
    "    \n",
    "\n",
    "    for _, row in tqdm(category_df.iterrows(), total=len(category_df)):\n",
    "        \n",
    "        disc_init = get_initial_discriminator_probs(row, model_d, tokenizer_d)\n",
    "        disc_init_policy.append(disc_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  \n",
    "        gen_init = get_initial_generator_probs(row, model_d, tokenizer_d)\n",
    "        \n",
    "        gen_init_policy.append(gen_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get))\n",
    "       \n",
    "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
    "        \n",
    "        candidates =  [f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])]\n",
    "\n",
    "\n",
    "        gen_final, disc_final = equilibrium_search(\n",
    "            gen_init, disc_init, candidates,\n",
    "            T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
    "        )\n",
    "        disc_final_policy_consensus.append( disc_final)\n",
    "        gen_final_policy_consensus.append(gen_final)\n",
    "\n",
    "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
    "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
    "        \n",
    "        gen_answer.append(best_answer_g)\n",
    "        disc_answer.append(best_answer_d)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
    "    category_df[\"disc_answer\"] = disc_answer\n",
    "    category_df[\"gen_answer\"] = gen_answer\n",
    "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
    "    category_df[\"disc_final_policy_consensus\"] = disc_final_policy_consensus\n",
    "    category_df[\"disc_init_policy\"] = disc_init_policy\n",
    "    category_df[\"gen_init_policy\"] = gen_init_policy\n",
    "    category_df[\"gen_final_policy_consensus\"] = gen_final_policy_consensus\n",
    "\n",
    "    \n",
    "    return category_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe078ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/t/toz015/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/u/home/t/toz015/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a5911356a74b8cafc5b0e27ed108ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a803f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 448/448 [04:14<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp_df_llama2 = subcategory_df_function(model_d, tokenizer_d, gpqa_df)\n",
    "\n",
    "file_path = 'Data/gpqa_policy_df_Llama2_7b\".csv'\n",
    "temp_df_llama2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4efcef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc882fcefa0444f4b21b002126d17529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 448/448 [05:24<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-13b-hf\")\n",
    "\n",
    "\n",
    "temp_df_qwen = subcategory_df_function(model_d, tokenizer_d, gpqa_df)\n",
    "\n",
    "\n",
    "file_path = 'Data/gpqa_policy_df_Llama2_13b.csv'\n",
    "temp_df_qwen.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "180ffaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bb200d3692429cb5cad805e6187810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 448/448 [05:33<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_d, tokenizer_d = load_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "temp_df_oqwen_arc_easy = subcategory_df_function( model_d, tokenizer_d, gpqa_df)\n",
    "\n",
    "file_path = 'Data/gpqa_policy_df_oqwen_7B.csv'\n",
    "temp_df_oqwen_arc_easy.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
