{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.30.0\n",
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Token: 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\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>subject</th>\n",
       "      <th>abstract_algebra</th>\n",
       "      <th>anatomy</th>\n",
       "      <th>astronomy</th>\n",
       "      <th>business_ethics</th>\n",
       "      <th>clinical_knowledge</th>\n",
       "      <th>college_biology</th>\n",
       "      <th>college_chemistry</th>\n",
       "      <th>college_computer_science</th>\n",
       "      <th>college_mathematics</th>\n",
       "      <th>college_medicine</th>\n",
       "      <th>...</th>\n",
       "      <th>professional_accounting</th>\n",
       "      <th>professional_law</th>\n",
       "      <th>professional_medicine</th>\n",
       "      <th>professional_psychology</th>\n",
       "      <th>public_relations</th>\n",
       "      <th>security_studies</th>\n",
       "      <th>sociology</th>\n",
       "      <th>us_foreign_policy</th>\n",
       "      <th>virology</th>\n",
       "      <th>world_religions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>100</td>\n",
       "      <td>135</td>\n",
       "      <td>145</td>\n",
       "      <td>100</td>\n",
       "      <td>264</td>\n",
       "      <td>144</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>281</td>\n",
       "      <td>1534</td>\n",
       "      <td>272</td>\n",
       "      <td>608</td>\n",
       "      <td>108</td>\n",
       "      <td>244</td>\n",
       "      <td>201</td>\n",
       "      <td>99</td>\n",
       "      <td>166</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "subject   abstract_algebra  anatomy  astronomy  business_ethics  \\\n",
       "question               100      135        145              100   \n",
       "\n",
       "subject   clinical_knowledge  college_biology  college_chemistry  \\\n",
       "question                 264              144                 99   \n",
       "\n",
       "subject   college_computer_science  college_mathematics  college_medicine  \\\n",
       "question                       100                   99                95   \n",
       "\n",
       "subject   ...  professional_accounting  professional_law  \\\n",
       "question  ...                      281              1534   \n",
       "\n",
       "subject   professional_medicine  professional_psychology  public_relations  \\\n",
       "question                    272                      608               108   \n",
       "\n",
       "subject   security_studies  sociology  us_foreign_policy  virology  \\\n",
       "question               244        201                 99       166   \n",
       "\n",
       "subject   world_religions  \n",
       "question              171  \n",
       "\n",
       "[1 rows x 57 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_***REDACTED***\"\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "import accelerate\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the dataset split\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "\n",
    "mmlu_df = dataset.to_pandas()\n",
    "mmlu_df = mmlu_df.drop_duplicates(subset=['question'])\n",
    "mmlu_df[\"question\"].groupby(mmlu_df[\"subject\"]).count().to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ec8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "subject_to_category = {}\n",
    "for category, subjects in categories.items():\n",
    "    for subject in subjects:\n",
    "        subject_to_category[subject] = category\n",
    "        \n",
    "\n",
    "mmlu_df[\"subcategories\"] = mmlu_df[\"subject\"].apply(lambda x: subcategories[x][0])\n",
    "mmlu_df[\"categories\"] = mmlu_df[\"subcategories\"].apply(lambda x: subject_to_category.get(x))\n",
    "\n",
    "file_path = '/u/home/t/toz015/Agent/mmlu_df.csv'\n",
    "mmlu_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdd7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biology</th>\n",
       "      <td>453</td>\n",
       "      <td>453</td>\n",
       "      <td>453</td>\n",
       "      <td>453</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>436</td>\n",
       "      <td>436</td>\n",
       "      <td>436</td>\n",
       "      <td>436</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chemistry</th>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer science</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>730</td>\n",
       "      <td>730</td>\n",
       "      <td>730</td>\n",
       "      <td>730</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geography</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>1554</td>\n",
       "      <td>1554</td>\n",
       "      <td>1554</td>\n",
       "      <td>1554</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>history</th>\n",
       "      <td>928</td>\n",
       "      <td>928</td>\n",
       "      <td>928</td>\n",
       "      <td>928</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>1761</td>\n",
       "      <td>1761</td>\n",
       "      <td>1761</td>\n",
       "      <td>1761</td>\n",
       "      <td>1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>1053</td>\n",
       "      <td>1053</td>\n",
       "      <td>1053</td>\n",
       "      <td>1053</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>1164</td>\n",
       "      <td>1164</td>\n",
       "      <td>1164</td>\n",
       "      <td>1164</td>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philosophy</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics</th>\n",
       "      <td>611</td>\n",
       "      <td>611</td>\n",
       "      <td>611</td>\n",
       "      <td>611</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question  subject  choices  answer  categories\n",
       "subcategories                                                   \n",
       "biology                453      453      453     453         453\n",
       "business               436      436      436     436         436\n",
       "chemistry              297      297      297     297         297\n",
       "computer science       410      410      410     410         410\n",
       "culture                332      332      332     332         332\n",
       "economics              730      730      730     730         730\n",
       "engineering            145      145      145     145         145\n",
       "geography              198      198      198     198         198\n",
       "health                1554     1554     1554    1554        1554\n",
       "history                928      928      928     928         928\n",
       "law                   1761     1761     1761    1761        1761\n",
       "math                  1053     1053     1053    1053        1053\n",
       "other                 1164     1164     1164    1164        1164\n",
       "philosophy            2011     2011     2011    2011        2011\n",
       "physics                611      611      611     611         611\n",
       "politics               644      644      644     644         644\n",
       "psychology            1142     1142     1142    1142        1142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu_df.groupby(\"subcategories\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c13cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# 1. INITIAL GENERATOR POLICIES\n",
    "################################\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_generator_prompt(\n",
    "    subject,\n",
    "    target_question,\n",
    "    target_choices,\n",
    "    get_correct\n",
    "):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject))\n",
    "\n",
    "    prompt += f\"{target_question}\"\n",
    "    for i, c in enumerate(target_choices):\n",
    "        prompt += \"\\n{}\".format(c)\n",
    "        \n",
    "    if get_correct:\n",
    "        prompt += \"\\nAnswer:\"\n",
    "    else:\n",
    "        prompt += \"\\nIncorrect Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_logits = []\n",
    "    for letter in choices:\n",
    "        token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
    "        choice_logits.append(logits[token_id].item())\n",
    "    \n",
    "    \n",
    "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    choice_probs =  {choice: prob for choice, prob in zip(choices, probs)}\n",
    "    \n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "\n",
    "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
    "    # Generate the letter answer\n",
    "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
    "\n",
    "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
    "    \n",
    "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
    "    \n",
    "    return probs \n",
    "\n",
    "\n",
    "\n",
    "def get_initial_generator_probs(row, model, tokenizer):\n",
    "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
    "    for v in [True, False]:\n",
    "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
    "        if v:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"correct\"][key] = val\n",
    "        else:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"incorrect\"][key] = val\n",
    "\n",
    "    return gen_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de33cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 2. INITIAL DISCRIMINATOR POLICIES\n",
    "###################################\n",
    "\n",
    "def build_discriminator_prompt(\n",
    "    subject: str,\n",
    "    question: str,\n",
    "    proposed_answer: str\n",
    ") -> str:\n",
    "    \"\"\"Builds a prompt to evaluate answer correctness.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    \n",
    "def get_discriminator_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    choices_list\n",
    "):\n",
    "    input_ids = input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    choice_logits = torch.tensor(\n",
    "        [\n",
    "            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "        ]\n",
    "    ).float()\n",
    "    \n",
    "    disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_probs =  {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
    "\n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "def evaluate_answer_correctness(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"Evaluates all possible answers for a question.\"\"\"\n",
    "    subject = row[\"subject\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for idx, answer in enumerate(choices):\n",
    "        prompt = build_discriminator_prompt(\n",
    "            subject=subject,\n",
    "            question=question,\n",
    "            proposed_answer=f\"{answer}\"\n",
    "        )\n",
    "        \n",
    "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
    "        \n",
    "        \n",
    "        disc_dict_answer =  {i: f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])}\n",
    "        \n",
    "        \n",
    "        results[disc_dict_answer[idx]] = probs\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "def get_initial_discriminator_probs(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
    "    \n",
    "\n",
    "    return disc_init\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8675f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
    "    \"\"\"\n",
    "    method='generator': pick argmax_y pi_G(correct|y)\n",
    "    method='discriminator': pick argmax_y pi_D(correct|y)\n",
    "    \"\"\"\n",
    "    if method == \"generator\":\n",
    "        # For each candidate y, we look at gen[\"correct\"][y].\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen[\"correct\"][y]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "    else:\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc[y][\"correct\"]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(arr):\n",
    "    \"\"\"Numerically stable softmax over a 1D numpy array.\"\"\"\n",
    "    m = np.max(arr)\n",
    "    exp_vals = np.exp(arr - m)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "\n",
    "def equilibrium_search(gen_init, disc_init, \n",
    "                       candidates, \n",
    "                       T=5000, \n",
    "                       eta_G=0.1, eta_D=0.1, \n",
    "                       lam_G=0.1, lam_D=0.01):\n",
    "    \"\"\"\n",
    "    Runs iterative no-regret policy updates to find approximate equilibrium.\n",
    "    gen_init, disc_init: dictionary form from the above initialization steps.\n",
    "    \"\"\"\n",
    "\n",
    "    gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "    disc = {}\n",
    "    for y in candidates:\n",
    "        disc[y] = dict(disc_init[y])\n",
    "\n",
    "    Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        # 1) Update Q\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            for y in candidates:\n",
    "                \n",
    "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "        for y in candidates:\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                \n",
    "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "        # 2) Update generator policy\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            logits = []\n",
    "            for y in candidates:\n",
    "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12) )/ (1/eta_G  + lam_G)\n",
    "                logits.append(val)\n",
    "\n",
    "            new_probs = softmax(np.array(logits))\n",
    "\n",
    "            for i, y in enumerate(candidates):\n",
    "                gen[v][y] = new_probs[i]\n",
    "            \n",
    "        # 3) Update discriminator policy\n",
    "        # Compute logits for all candidates under the same class\n",
    "        logits_correct = []\n",
    "        logits_incorrect = []\n",
    "        for y in candidates:\n",
    "            # Logit for \"correct\"\n",
    "            val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_correct.append(val_correct)\n",
    "\n",
    "            # Logit for \"incorrect\"\n",
    "            val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_incorrect.append(val_incorrect)\n",
    "\n",
    "        new_probs_correct = softmax(np.array(logits_correct))\n",
    "        new_probs_incorrect = softmax(np.array(logits_incorrect))\n",
    "\n",
    "        for i, y in enumerate(candidates):\n",
    "            disc[y][\"correct\"] = new_probs_correct[i]\n",
    "            disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
    "\n",
    "    return gen, disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf89481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load one model at a time with 4-bit quantization\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9efebe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcategory_df_function(model_d, tokenizer_d):\n",
    "    category_df = mmlu_df.copy()\n",
    "\n",
    "    gen_answer = []\n",
    "    disc_answer = []\n",
    "\n",
    "    gen_init_answer = []\n",
    "    disc_init_answer = []\n",
    "    corr_answer = []\n",
    "\n",
    "    for _, row in tqdm(category_df.iterrows(), total=len(category_df)):\n",
    "\n",
    "        disc_init = get_initial_discriminator_probs(row, model_d, tokenizer_d)\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  \n",
    "        gen_init = get_initial_generator_probs(row, model_d, tokenizer_d)\n",
    "        \n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        candidates_answer = [ choice for i, choice in enumerate(row[\"choices\"])]\n",
    "        corr_answer.append(candidates_answer[row[\"answer\"]-1])\n",
    "        \n",
    "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get)) \n",
    "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
    "        \n",
    "        candidates =  [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "        gen_final, disc_final = equilibrium_search(\n",
    "            gen_init, disc_init, candidates,\n",
    "            T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
    "        )\n",
    "\n",
    "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
    "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
    "        \n",
    "        gen_answer.append(best_answer_g)\n",
    "        disc_answer.append(best_answer_d)\n",
    "    \n",
    "    \n",
    "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
    "    category_df[\"corr_answer\"] = corr_answer\n",
    "    category_df[\"disc_answer\"] = disc_answer\n",
    "    category_df[\"gen_answer\"] = gen_answer\n",
    "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
    "\n",
    "    return category_df\n",
    "\n",
    "def subcategory_df_result_function(df):\n",
    "\n",
    "    dict_map = {0:\"A\", 1:\"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "    df_nrow = df.shape[0]\n",
    "\n",
    "    df[\"answer_letter\"] = df[\"answer\"].map(dict_map)\n",
    "\n",
    "    init_comb = (df[\"gen_init_answer\"] == df[\"disc_init_answer\"]).sum()\n",
    "    init_corr_comb = ((df[\"gen_init_answer\"] == df[\"disc_init_answer\"]) & (df[\"gen_init_answer\"] == df[\"answer_letter\"])).sum()\n",
    "\n",
    "    comb = (df[\"gen_answer\"] == df[\"disc_answer\"]).sum()\n",
    "    corr_comb = ((df[\"gen_answer\"] == df[\"disc_answer\"]) & (df[\"gen_answer\"] == df[\"answer_letter\"])).sum()\n",
    "\n",
    "    init_gen_corr = (df[\"gen_init_answer\"] == df[\"answer_letter\"]).sum()\n",
    "    gen_corr = (df[\"gen_answer\"] == df[\"answer_letter\"]).sum()\n",
    "\n",
    "    init_disc_corr = (df[\"disc_init_answer\"] == df[\"answer_letter\"]).sum()\n",
    "    disc_corr = (df[\"disc_answer\"] == df[\"answer_letter\"]).sum()\n",
    "\n",
    "    results = {\n",
    "    \"Metric\": [\n",
    "        \"Total Question #\",\n",
    "        \"Initial correct answers for G\",\n",
    "        \"Final correct answers for G\",\n",
    "        \"Initial correct answers for D\",\n",
    "        \"Final correct answers for D\",\n",
    "        \"Initial combined agreement\",\n",
    "        \"Final combined agreement\",\n",
    "\n",
    "        \"Initial combined correct agreement\",\n",
    "        \"Final combined correct agreement\"\n",
    "    ],\n",
    "    \"Count\": [\n",
    "        df_nrow,\n",
    "        init_gen_corr,\n",
    "        gen_corr,\n",
    "        init_disc_corr,\n",
    "        disc_corr,\n",
    "        init_comb,\n",
    "        comb,\n",
    "        init_corr_comb,\n",
    "        corr_comb\n",
    "\n",
    "    ]\n",
    "    }\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    results_df[\"Percentage\"] = (results_df[\"Count\"] / df_nrow).apply(lambda x: f\"{x:.0%}\")\n",
    "\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e637c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e1c86a159544f1b1c8b84062510c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 13869/13869 [1:55:37<00:00,  2.00it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Initial correct answers for G</td>\n",
       "      <td>6978</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final correct answers for G</td>\n",
       "      <td>7000</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Initial correct answers for D</td>\n",
       "      <td>6411</td>\n",
       "      <td>46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final correct answers for D</td>\n",
       "      <td>6966</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Initial combined agreement</td>\n",
       "      <td>7393</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Final combined agreement</td>\n",
       "      <td>13339</td>\n",
       "      <td>96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Initial combined correct agreement</td>\n",
       "      <td>4613</td>\n",
       "      <td>33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Final combined correct agreement</td>\n",
       "      <td>6809</td>\n",
       "      <td>49%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Percentage\n",
       "0                    Total Question #  13869       100%\n",
       "1       Initial correct answers for G   6978        50%\n",
       "2         Final correct answers for G   7000        50%\n",
       "3       Initial correct answers for D   6411        46%\n",
       "4         Final correct answers for D   6966        50%\n",
       "5          Initial combined agreement   7393        53%\n",
       "6            Final combined agreement  13339        96%\n",
       "7  Initial combined correct agreement   4613        33%\n",
       "8    Final combined correct agreement   6809        49%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-13b-hf\")\n",
    "\n",
    "temp_df_llama2 = subcategory_df_function(model_d, tokenizer_d)\n",
    "\n",
    "file_path = 'Data/mmlu_df_deepseek_Llama2_13b.csv'\n",
    "temp_df_llama2.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "llama2_df_result = subcategory_df_result_function(temp_df_llama2)\n",
    "display(llama2_df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4efcef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1278718643364b3ba4af6c15426ea7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 13869/13869 [1:54:43<00:00,  2.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Initial correct answers for G</td>\n",
       "      <td>7404</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final correct answers for G</td>\n",
       "      <td>7453</td>\n",
       "      <td>54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Initial correct answers for D</td>\n",
       "      <td>5878</td>\n",
       "      <td>42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final correct answers for D</td>\n",
       "      <td>7430</td>\n",
       "      <td>54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Initial combined agreement</td>\n",
       "      <td>6119</td>\n",
       "      <td>44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Final combined agreement</td>\n",
       "      <td>13175</td>\n",
       "      <td>95%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Initial combined correct agreement</td>\n",
       "      <td>4063</td>\n",
       "      <td>29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Final combined correct agreement</td>\n",
       "      <td>7226</td>\n",
       "      <td>52%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Percentage\n",
       "0                    Total Question #  13869       100%\n",
       "1       Initial correct answers for G   7404        53%\n",
       "2         Final correct answers for G   7453        54%\n",
       "3       Initial correct answers for D   5878        42%\n",
       "4         Final correct answers for D   7430        54%\n",
       "5          Initial combined agreement   6119        44%\n",
       "6            Final combined agreement  13175        95%\n",
       "7  Initial combined correct agreement   4063        29%\n",
       "8    Final combined correct agreement   7226        52%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model_d\n",
    "del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "temp_df_qwen = subcategory_df_function(model_d, tokenizer_d)\n",
    "\n",
    "\n",
    "file_path = 'Data/mmlu_df_Llama2_7b\".csv'\n",
    "temp_df_qwen.to_csv(file_path, index=False)\n",
    "\n",
    "qwen_df_result = subcategory_df_result_function(temp_df_qwen)\n",
    "display(qwen_df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05c16c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/t/toz015/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/u/home/t/toz015/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cda7b8a3c5f40ac8c1426dda9f9a6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 13869/13869 [1:52:28<00:00,  2.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Initial correct answers for G</td>\n",
       "      <td>9849</td>\n",
       "      <td>71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final correct answers for G</td>\n",
       "      <td>9838</td>\n",
       "      <td>71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Initial correct answers for D</td>\n",
       "      <td>8698</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final correct answers for D</td>\n",
       "      <td>9761</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Initial combined agreement</td>\n",
       "      <td>9515</td>\n",
       "      <td>69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Final combined agreement</td>\n",
       "      <td>13370</td>\n",
       "      <td>96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Initial combined correct agreement</td>\n",
       "      <td>7667</td>\n",
       "      <td>55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Final combined correct agreement</td>\n",
       "      <td>9607</td>\n",
       "      <td>69%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Percentage\n",
       "0                    Total Question #  13869       100%\n",
       "1       Initial correct answers for G   9849        71%\n",
       "2         Final correct answers for G   9838        71%\n",
       "3       Initial correct answers for D   8698        63%\n",
       "4         Final correct answers for D   9761        70%\n",
       "5          Initial combined agreement   9515        69%\n",
       "6            Final combined agreement  13370        96%\n",
       "7  Initial combined correct agreement   7667        55%\n",
       "8    Final combined correct agreement   9607        69%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model_d \n",
    "del tokenizer_d \n",
    "\n",
    "model_d, tokenizer_d = load_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "\n",
    "temp_df_oqwen = subcategory_df_function( model_d, tokenizer_d)\n",
    "\n",
    "file_path = 'Data/mmlu_df_oqwen_7B.csv'\n",
    "temp_df_oqwen.to_csv(file_path, index=False)\n",
    "\n",
    "oqwen_df_result = subcategory_df_result_function(temp_df_oqwen)\n",
    "display(oqwen_df_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5418076c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oqwen correct answers</td>\n",
       "      <td>8698</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen correct answers</td>\n",
       "      <td>5878</td>\n",
       "      <td>42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2 correct answers</td>\n",
       "      <td>6411</td>\n",
       "      <td>46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen llama2 oqwen</td>\n",
       "      <td>2957</td>\n",
       "      <td>21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen oqwen</td>\n",
       "      <td>4443</td>\n",
       "      <td>32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qwen llama2</td>\n",
       "      <td>3264</td>\n",
       "      <td>24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oqwen llama2</td>\n",
       "      <td>5245</td>\n",
       "      <td>38%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Count Accuracy\n",
       "0        Total Question #  13869     100%\n",
       "1   oqwen correct answers   8698      63%\n",
       "2    qwen correct answers   5878      42%\n",
       "3  llama2 correct answers   6411      46%\n",
       "4       qwen llama2 oqwen   2957      21%\n",
       "5              qwen oqwen   4443      32%\n",
       "6             qwen llama2   3264      24%\n",
       "7            oqwen llama2   5245      38%"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict_map = {0:\"A\", 1:\"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "\n",
    "temp_df_qwen[\"answer_letter\"] = temp_df_qwen[\"answer\"].map(dict_map)\n",
    "temp_df_oqwen[\"answer_letter\"] = temp_df_oqwen[\"answer\"].map(dict_map)\n",
    "temp_df_llama2[\"answer_letter\"] = temp_df_llama2[\"answer\"].map(dict_map)\n",
    "\n",
    "total_n = temp_df_oqwen.shape[0]\n",
    "\n",
    "qwen_corr =  (temp_df_qwen[\"disc_init_answer\"] == temp_df_qwen[\"answer_letter\"]).sum()\n",
    "oqwen_corr = (temp_df_oqwen[\"disc_init_answer\"] == temp_df_oqwen[\"answer_letter\"]).sum()\n",
    "llama2_corr = (temp_df_llama2[\"disc_init_answer\"] == temp_df_llama2[\"answer_letter\"]).sum()\n",
    "\n",
    "qwen_oqwen =  ((temp_df_qwen[\"disc_init_answer\"] == temp_df_oqwen[\"disc_init_answer\"]) & (temp_df_oqwen[\"disc_init_answer\"] == temp_df_oqwen[\"answer_letter\"])).sum()\n",
    "qwen_llama2 =  ((temp_df_qwen[\"disc_init_answer\"] == temp_df_llama2[\"disc_init_answer\"]) & (temp_df_llama2[\"disc_init_answer\"] == temp_df_llama2[\"answer_letter\"])).sum()\n",
    "oqwen_llama2 =  ((temp_df_oqwen[\"disc_init_answer\"] == temp_df_llama2[\"disc_init_answer\"]) & (temp_df_llama2[\"disc_init_answer\"] == temp_df_llama2[\"answer_letter\"])).sum()\n",
    "qwen_llama2_oqwen = ((temp_df_oqwen[\"disc_init_answer\"] == temp_df_llama2[\"disc_init_answer\"]) & (temp_df_llama2[\"disc_init_answer\"] == temp_df_llama2[\"answer_letter\"]) & (temp_df_qwen[\"disc_init_answer\"] == temp_df_oqwen[\"disc_init_answer\"])).sum()\n",
    "\n",
    "\n",
    "results_d = {\n",
    "\"Metric\": [\n",
    "    \"Total Question #\",\n",
    "    \"oqwen correct answers\",\n",
    "    \"qwen correct answers\",\n",
    "    \"llama2 correct answers\",\n",
    "    \"qwen llama2 oqwen\", \n",
    "    \"qwen oqwen\",\n",
    "    \"qwen llama2\",\n",
    "    \"oqwen llama2\",\n",
    "],\n",
    "\"Count\": [\n",
    "    total_n,\n",
    "    oqwen_corr,\n",
    "    qwen_corr,\n",
    "    llama2_corr,\n",
    "    qwen_llama2_oqwen,\n",
    "    qwen_oqwen,\n",
    "    qwen_llama2,\n",
    "    oqwen_llama2,\n",
    "]  \n",
    "}\n",
    "   \n",
    "results_d = pd.DataFrame(results_d)\n",
    "results_d[\"Accuracy\"] = (results_d[\"Count\"] / total_n).apply(lambda x: f\"{x:.0%}\")\n",
    "results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e19aa928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Question #</td>\n",
       "      <td>13869</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oqwen correct answers</td>\n",
       "      <td>9849</td>\n",
       "      <td>71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen correct answers</td>\n",
       "      <td>7404</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2 correct answers</td>\n",
       "      <td>6978</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen llama2 oqwen</td>\n",
       "      <td>4308</td>\n",
       "      <td>31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen oqwen</td>\n",
       "      <td>6482</td>\n",
       "      <td>47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qwen llama2</td>\n",
       "      <td>4610</td>\n",
       "      <td>33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oqwen llama2</td>\n",
       "      <td>5840</td>\n",
       "      <td>42%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Count Accuracy\n",
       "0        Total Question #  13869     100%\n",
       "1   oqwen correct answers   9849      71%\n",
       "2    qwen correct answers   7404      53%\n",
       "3  llama2 correct answers   6978      50%\n",
       "4       qwen llama2 oqwen   4308      31%\n",
       "5              qwen oqwen   6482      47%\n",
       "6             qwen llama2   4610      33%\n",
       "7            oqwen llama2   5840      42%"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_n = temp_df_oqwen.shape[0]\n",
    "\n",
    "qwen_corr =  (temp_df_qwen[\"gen_init_answer\"] == temp_df_qwen[\"answer_letter\"]).sum()\n",
    "oqwen_corr = (temp_df_oqwen[\"gen_init_answer\"] == temp_df_oqwen[\"answer_letter\"]).sum()\n",
    "llama2_corr = (temp_df_llama2[\"gen_init_answer\"] == temp_df_llama2[\"answer_letter\"]).sum()\n",
    "\n",
    "qwen_oqwen =  ((temp_df_qwen[\"gen_init_answer\"] == temp_df_oqwen[\"gen_init_answer\"]) & (temp_df_oqwen[\"gen_init_answer\"] == temp_df_oqwen[\"answer_letter\"])).sum()\n",
    "qwen_llama2 =  ((temp_df_qwen[\"gen_init_answer\"] == temp_df_llama2[\"gen_init_answer\"]) & (temp_df_llama2[\"gen_init_answer\"] == temp_df_llama2[\"answer_letter\"])).sum()\n",
    "oqwen_llama2 =  ((temp_df_oqwen[\"gen_init_answer\"] == temp_df_llama2[\"gen_init_answer\"]) & (temp_df_llama2[\"gen_init_answer\"] == temp_df_llama2[\"answer_letter\"])).sum()\n",
    "qwen_llama2_oqwen = ((temp_df_oqwen[\"gen_init_answer\"] == temp_df_llama2[\"gen_init_answer\"]) & (temp_df_llama2[\"gen_init_answer\"] == temp_df_llama2[\"answer_letter\"]) & (temp_df_qwen[\"gen_init_answer\"] == temp_df_oqwen[\"gen_init_answer\"])).sum()\n",
    "\n",
    "\n",
    "results_d = {\n",
    "\"Metric\": [\n",
    "    \"Total Question #\",\n",
    "    \"oqwen correct answers\",\n",
    "    \"qwen correct answers\",\n",
    "    \"llama2 correct answers\",\n",
    "    \"qwen llama2 oqwen\", \n",
    "    \"qwen oqwen\",\n",
    "    \"qwen llama2\",\n",
    "    \"oqwen llama2\",\n",
    "],\n",
    "\"Count\": [\n",
    "    total_n,\n",
    "    oqwen_corr,\n",
    "    qwen_corr,\n",
    "    llama2_corr,\n",
    "    qwen_llama2_oqwen,\n",
    "    qwen_oqwen,\n",
    "    qwen_llama2,\n",
    "    oqwen_llama2,\n",
    "]  \n",
    "}\n",
    "   \n",
    "results_d = pd.DataFrame(results_d)\n",
    "results_d[\"Accuracy\"] = (results_d[\"Count\"] / total_n).apply(lambda x: f\"{x:.0%}\")\n",
    "results_d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
