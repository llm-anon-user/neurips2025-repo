{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"hf_***REDACTED***\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "hf_token = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ec8b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170, 4)\n",
      "Index(['id', 'question', 'choices', 'answerKey'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "\n",
    "arc_df = arc_data.to_pandas()\n",
    "arc_df = arc_df.drop_duplicates(subset=['question'])\n",
    "print(arc_df.shape)\n",
    "print(arc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdd7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "      <th>choices_dic</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mercury_7175875</td>\n",
       "      <td>An astronomer observes that a planet rotates f...</td>\n",
       "      <td>[Planetary density will decrease., Planetary y...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'text': ['Planetary density will decrease.', ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mercury_SC_409171</td>\n",
       "      <td>A group of engineers wanted to know how differ...</td>\n",
       "      <td>[buildings will be built faster, buildings wil...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'text': ['buildings will be built faster', 'b...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           question  \\\n",
       "0    Mercury_7175875  An astronomer observes that a planet rotates f...   \n",
       "1  Mercury_SC_409171  A group of engineers wanted to know how differ...   \n",
       "\n",
       "                                             choices answerKey  \\\n",
       "0  [Planetary density will decrease., Planetary y...         C   \n",
       "1  [buildings will be built faster, buildings wil...         B   \n",
       "\n",
       "                                         choices_dic  subject  \n",
       "0  {'text': ['Planetary density will decrease.', ...  science  \n",
       "1  {'text': ['buildings will be built faster', 'b...  science  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_df[\"choices_dic\"] = arc_df[\"choices\"]\n",
    "arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "arc_df[\"subject\"] = \"science\"\n",
    "arc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3c5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "import accelerate\n",
    "print(accelerate.__version__)  # Should show â‰¥0.26.0\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c13cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# 1. INITIAL GENERATOR POLICIES\n",
    "################################\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_generator_prompt(\n",
    "    subject,\n",
    "    target_question,\n",
    "    target_choices,\n",
    "    get_correct\n",
    "):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject))\n",
    "\n",
    "    prompt += f\"{target_question}\"\n",
    "    for i, c in enumerate(target_choices):\n",
    "        prompt += \"\\n{}\".format(c)\n",
    "        \n",
    "    if get_correct:\n",
    "        prompt += \"\\nAnswer:\"\n",
    "    else:\n",
    "        prompt += \"\\nIncorrect Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_logits = []\n",
    "    for letter in choices:\n",
    "        token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
    "        choice_logits.append(logits[token_id].item())\n",
    "    \n",
    "    \n",
    "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    choice_probs =  {choice: prob for choice, prob in zip(choices, probs)}\n",
    "    \n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "\n",
    "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
    "    # Generate the letter answer\n",
    "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
    "\n",
    "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
    "    \n",
    "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
    "    \n",
    "    return probs \n",
    "\n",
    "\n",
    "\n",
    "def get_initial_generator_probs(row, model, tokenizer):\n",
    "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
    "    for v in [True, False]:\n",
    "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
    "        if v:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"correct\"][key] = val\n",
    "                #print(gen_init.items())\n",
    "        else:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"incorrect\"][key] = val\n",
    "\n",
    "    return gen_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de33cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 2. INITIAL DISCRIMINATOR POLICIES\n",
    "###################################\n",
    "\n",
    "def build_discriminator_prompt(\n",
    "    subject: str,\n",
    "    question: str,\n",
    "    proposed_answer: str\n",
    ") -> str:\n",
    "    \"\"\"Builds a prompt to evaluate answer correctness.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    \n",
    "def get_discriminator_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    choices_list\n",
    "):\n",
    "    input_ids = input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    choice_logits = torch.tensor(\n",
    "        [\n",
    "            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "        ]\n",
    "    ).float()\n",
    "    \n",
    "    disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_probs =  {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
    "\n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "def evaluate_answer_correctness(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"Evaluates all possible answers for a question.\"\"\"\n",
    "    subject = row[\"subject\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for idx, answer in enumerate(choices):\n",
    "        prompt = build_discriminator_prompt(\n",
    "            subject=subject,\n",
    "            question=question,\n",
    "            proposed_answer=f\"{answer}\"\n",
    "        )\n",
    "        \n",
    "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
    "        \n",
    "        \n",
    "        disc_dict_answer =  {i: f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])}\n",
    "        \n",
    "        \n",
    "        results[disc_dict_answer[idx]] = probs\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "def get_initial_discriminator_probs(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
    "    \n",
    "\n",
    "    return disc_init\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8675f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
    "    \"\"\"\n",
    "    method='generator': pick argmax_y pi_G(correct|y)\n",
    "    method='discriminator': pick argmax_y pi_D(correct|y)\n",
    "    \"\"\"\n",
    "    if method == \"generator\":\n",
    "        # For each candidate y, we look at gen[\"correct\"][y].\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen[\"correct\"][y]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "    else:\n",
    "        # method='discriminator'\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc[y][\"correct\"]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(arr):\n",
    "    \"\"\"Numerically stable softmax over a 1D numpy array.\"\"\"\n",
    "    m = np.max(arr)\n",
    "    exp_vals = np.exp(arr - m)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "\n",
    "def equilibrium_search(gen_init, disc_init, \n",
    "                       candidates, \n",
    "                       T=5000, \n",
    "                       eta_G=0.1, eta_D=0.1, \n",
    "                       lam_G=0.1, lam_D=0.01):\n",
    "    \"\"\"\n",
    "    Runs iterative no-regret policy updates to find approximate equilibrium.\n",
    "    gen_init, disc_init: dictionary form from the above initialization steps.\n",
    "    \"\"\"\n",
    "    # Convert these dicts into np arrays for speed if you like.\n",
    "    # But for clarity, we'll just keep dict form.\n",
    "\n",
    "    gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "    disc = {}\n",
    "    for y in candidates:\n",
    "        disc[y] = dict(disc_init[y])  # copy\n",
    "\n",
    "    Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        # 1) Update Q\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            for y in candidates:\n",
    "                \n",
    "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "        for y in candidates:\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                \n",
    "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "        # 2) Update generator policy\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            logits = []\n",
    "            for y in candidates:\n",
    "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12) )/ (1/eta_G  + lam_G)\n",
    "                logits.append(val)\n",
    "\n",
    "            new_probs = softmax(np.array(logits))\n",
    "\n",
    "            for i, y in enumerate(candidates):\n",
    "                gen[v][y] = new_probs[i]\n",
    "        logits_correct = []\n",
    "        logits_incorrect = []\n",
    "        for y in candidates:\n",
    "            # Logit for \"correct\"\n",
    "            val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_correct.append(val_correct)\n",
    "\n",
    "            # Logit for \"incorrect\"\n",
    "            val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_incorrect.append(val_incorrect)\n",
    "\n",
    "        # Apply softmax across all candidates for each class\n",
    "        new_probs_correct = softmax(np.array(logits_correct))\n",
    "        new_probs_incorrect = softmax(np.array(logits_incorrect))\n",
    "\n",
    "\n",
    "        for i, y in enumerate(candidates):\n",
    "            disc[y][\"correct\"] = new_probs_correct[i]\n",
    "            disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
    "\n",
    "    return gen, disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf89481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load one model at a time with 4-bit quantization\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efebe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subcategory_df_function(model_d, tokenizer_d, df):\n",
    "    \n",
    "    category_df = df.copy()\n",
    "\n",
    "    gen_answer = []\n",
    "    disc_answer = []\n",
    "    gen_init_answer = []\n",
    "    disc_init_answer = []\n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_final_policy_consensus = []\n",
    "    gen_final_policy_consensus = []\n",
    "    \n",
    "\n",
    "    for _, row in tqdm(category_df.iterrows(), total=len(category_df)):\n",
    "\n",
    "        disc_init = get_initial_discriminator_probs(row, model_d, tokenizer_d)\n",
    "        disc_init_policy.append(disc_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  \n",
    "        gen_init = get_initial_generator_probs(row, model_d, tokenizer_d)\n",
    "        \n",
    "        gen_init_policy.append(gen_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get))\n",
    "       \n",
    "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
    "        \n",
    "        candidates =  [f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])]\n",
    "\n",
    "\n",
    "        gen_final, disc_final = equilibrium_search(\n",
    "            gen_init, disc_init, candidates,\n",
    "            T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
    "        )\n",
    "        disc_final_policy_consensus.append( disc_final)\n",
    "        gen_final_policy_consensus.append(gen_final)\n",
    "\n",
    "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
    "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
    "        \n",
    "        gen_answer.append(best_answer_g)\n",
    "        disc_answer.append(best_answer_d)\n",
    "    \n",
    "    \n",
    "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
    "    category_df[\"disc_answer\"] = disc_answer\n",
    "    category_df[\"gen_answer\"] = gen_answer\n",
    "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
    "    category_df[\"disc_final_policy_consensus\"] = disc_final_policy_consensus\n",
    "    category_df[\"disc_init_policy\"] = disc_init_policy\n",
    "    category_df[\"gen_init_policy\"] = gen_init_policy\n",
    "    category_df[\"gen_final_policy_consensus\"] = gen_final_policy_consensus\n",
    "\n",
    "    \n",
    "    return category_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe078ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a803f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1170/1170 [09:23<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp_df_llama2 = subcategory_df_function(model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = 'Data/arc_policy_df_Llama2_13b.csv'\n",
    "temp_df_llama2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4efcef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b4ed94bbee4ea2b62336ab7ee6ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1170/1170 [09:12<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model_d\n",
    "del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "temp_df_qwen = subcategory_df_function(model_d, tokenizer_d, arc_df)\n",
    "\n",
    "\n",
    "file_path = 'Data/arc_policy_df_Llama2_7b.csv'\n",
    "temp_df_qwen.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b05c16c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7223f3048008443c8543301d679f98c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1170/1170 [09:15<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model_d \n",
    "del tokenizer_d \n",
    "\n",
    "model_d, tokenizer_d = load_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "temp_df_oqwen = subcategory_df_function( model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = 'Data/arc_policy_df_oqwen_7B.csv'\n",
    "temp_df_oqwen.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3a7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2371, 4)\n",
      "Index(['id', 'question', 'choices', 'answerKey'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "      <th>choices_dic</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mercury_417466</td>\n",
       "      <td>Which statement best explains why photosynthes...</td>\n",
       "      <td>[Sunlight is the source of energy for nearly a...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'text': ['Sunlight is the source of energy fo...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mercury_7081673</td>\n",
       "      <td>Which piece of safety equipment is used to kee...</td>\n",
       "      <td>[safety goggles, breathing mask, rubber gloves...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'text': ['safety goggles', 'breathing mask', ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                           question  \\\n",
       "0   Mercury_417466  Which statement best explains why photosynthes...   \n",
       "1  Mercury_7081673  Which piece of safety equipment is used to kee...   \n",
       "\n",
       "                                             choices answerKey  \\\n",
       "0  [Sunlight is the source of energy for nearly a...         A   \n",
       "1  [safety goggles, breathing mask, rubber gloves...         B   \n",
       "\n",
       "                                         choices_dic  subject  \n",
       "0  {'text': ['Sunlight is the source of energy fo...  science  \n",
       "1  {'text': ['safety goggles', 'breathing mask', ...  science  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "arc_data_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split = \"test\")\n",
    "\n",
    "arc_df_easy = arc_data_easy.to_pandas()\n",
    "\n",
    "arc_df_easy = arc_df_easy.drop_duplicates(subset=['question'])\n",
    "arc_df_easy[\"choices_dic\"] = arc_df_easy[\"choices\"]\n",
    "arc_df_easy[\"choices\"] = arc_df_easy[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "arc_df_easy[\"subject\"] = \"science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f00378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d811a654660429a8d0702730fee1600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2371/2371 [18:36<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-2-7b-hf\")\n",
    "temp_df_llama2_arc_easy = subcategory_df_function(model_d, tokenizer_d, arc_df_easy)\n",
    "\n",
    "file_path = 'Data/arc_policy_df_easy_Llama2_7b.csv'\n",
    "temp_df_llama2_arc_easy.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9970ff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530e7a8f49374791b648fac5c256b14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2371/2371 [19:32<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del model_d \n",
    "del tokenizer_d \n",
    "\n",
    "model_d, tokenizer_d = load_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "\n",
    "temp_df_oqwen_arc_easy = subcategory_df_function( model_d, tokenizer_d, arc_df_easy)\n",
    "\n",
    "file_path = 'Data/arc_policy_df_easy_oqwen_7B.csv'\n",
    "temp_df_oqwen_arc_easy.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
